---
author: Roham Koohestani, Zofia Rogacka-Trojak, Antonio-Florin Lupu, Pranav Pisupati
group_number: 2
title: "Measuring the Energy Cost of Prompt Engineering for Software Engineering Tasks"
image: "img/g2_se_prompt.webp"
date: 12/02/2026
summary: |-
  Prompt engineering often improves answer quality, but it can also increase token usage, latency,
  and the number of model calls. This project proposes a reproducible experiment design to quantify
  the energy/performance trade-offs of common prompting strategies (e.g., step-by-step, politeness,
  and answer-only) on software engineering tasks using automated evaluation.
identifier: p1_measuring_software_2026 # Do not change this
all_projects_page: "../p1_measuring_software" # Do not change this
---

## 1. Motivation

Prompt engineering is widely used to improve LLM output quality. Techniques like asking for step-by-step explanations or politely asking for an answer are common practices among users. However, these patternssiginificantly change the number of tokens processed (both input and output), the end-to-end latency, and ultimately, the energy consumption of the inference process.

In software engineering (SE) workflows, where the use of LLM is a daily practice, prompting patterns are no longer just a user-interface choice. They can influence the sustainability of day-to-day engineering work at scale.

This post sets up the basis of an experimental methodology to measure the energy cost of prompt engineering for SE tasks, while keeping the experiment reproducible and statistically defensible.

### Inspiration (Related Work)
The idea of performing an experiment on the energy cost of prompting strategies was inspired by the following works:
- [Words to Watts: How Prompting Patterns Shape AI's Environmental Impact](https://www.capgemini.com/insights/expert-perspectives/from-words-to-watts-how-prompting-patterns-shape-ais-environmental-impact/)
- [Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering](https://arxiv.org/abs/2509.22320)
- [Prompt engineering and its implications on the energy consumption of Large Language Models](https://arxiv.org/html/2501.05899v1)

--- 

## 2. Research Questions
Through this experiment we aim answer the question of *How do common prompting strategies change the total energy consumed per solved SE task compared to a minimal baseline prompt?* Moreover, we are exploring whether those differences yield enough quality improvements to justify the energy cost.

---

## 3. Experimental Design
To ensure reproducible and reliable results we created a controlled experiment with the following components:

### 3.1 Prompting Strategies - Independent Variable
We treated the prompting strategy as the independent variable. We kept the model (`deepseek-ai/deepseek-coder-1.3b-instruct`), decoding parameters (temperature, max tokens), and evaluation harness strictly constant.

Each strategy corresponds to a specific way of framing the prompt, which can influence the model's response and the number of tokens generated.

1. `baseline_single_shot` - A concise instruction with no extra framing.
2. `polite_single_shot`: The baseline prompt with added politeness markers ("please", "thank you") that do not alter the core instruction.
3. `think_step_by_step`: Requests explicit step-by-step reasoning before outputting the final answer (designed to massively increase completion tokens).
4. `answer_only_no_expl`: Explicitly requests a short final answer with no explanation (designed to minimize completion tokens).

*Note: We avoid relying on hidden "chain-of-thought". For transparency and comparability, a condition is defined by observable prompt text and an observable output format requirement.*

#### 3.1.1 Prompt Templates

All conditions share the same core task payload (the HumanEval function signature and docstring) and only differ in the wrapper text.

We will use placeholders:

- `{TASK}`: the task instructions + condition appropriate prompt wrapper
- `{OUTPUT_SCHEMA}`: strict output requirement: Provide your answer in the following format:\n```python\n<your code here>\n```

**baseline_single_shot**

```
{TASK}
{OUTPUT_SCHEMA}
```

**polite_single_shot**

```
Please help with writing the following function.
{TASK}
{OUTPUT_SCHEMA}
Thanks!
```

**think_step_by_step**

```
{TASK}
Think step-by-step. First, write your reasoning. Then provide your final output in a python code block.
{OUTPUT_SCHEMA}
```

**answer_only_no_expl**

```
Do not provide explanations, complete the following function
{TASK}
{OUTPUT_SCHEMA}
```


### 3.2 Dataset and Collected Metrics
We utilized the **HumanEval** dataset, a standard benchmark for evaluating the functional correctness of code generated by LLMs. This allowed us to automatically verify if the generated code actually passed the required unit tests.

For hardware and energy measurement, we used **EnergiBridge** to log CPU and GPU power consumption. During execution, we logged:
- Timestamps for the start/end of inference.
- GPU and CPU power in milliwatts/Joules.
- `tokens_in` and `tokens_out` using the HuggingFace `AutoTokenizer`.

TODO: Add more metrics if you use them!!!!

### 3.3 Measurement Hygiene - Protocol to Reduce Bias
We follow common energy-measurement hygiene to reduce confounding, caused by factors such as background processes:
* **"Zen Mode" Execution:** This was our most critical operational rule. Before any benchmark started, the host machine was put into a strict "Zen mode." All background applications were closed, system notifications, WiFi, and Bluetooth were disabled.
* **Freeze Settings:** We fixed screen brightness, power mode, network type, and CPU/GPU power settings to ensure consistency across runs.
* **Warm-up Runs:** Before recording measurements, we performed warm-up runs to stabilize the model and the system.
* **Randomization:** We randomized the order of tasks and conditions to mitigate time-based drift in measurements.
* **Repetitions:** Each (task, condition) pair was executed multiple times to account for variability and ensure statistical significance.
* **Output Schemas:** We enforced strict output formats to reduce variance in verbosity, which can affect token counts and energy consumption.

---

## 4. Exploratory Data Analysis (EDA) and Results

### 4.1 Overall Energy Consumption
Firstly, we analyzed the average energy consumed per strategy across all tasks. As shown in Figure 1, the `think_step_by_step`
strategy consumed significantly more energy on average compared to the `answer_only_no_expl` strategy, which had the
lowest energy consumption. The `polite_single_shot` strategy showed a slight increase in energy consumption compared to
the `baseline_single_shot`. Nevertheless, the difference between the `think_step_by_step` and the other strategy was
the most pronounced, which prompts us to further investigate the underlying causes.

<p align="center">
  <img src="./img/g2_avg_energy.png" alt="Average Energy Bar Chart" width="600"/>
  <br>
  <i><b>Figure 1:</b> Average Energy Consumed by Prompting Strategy.</i>
</p>

### 4.2 Power Draw Over Time
To understand the cause of those differences, we analyzed the power draw over time for both CPU and GPU. As shown in
Figures 2a and 2b, the `think_step_by_step` strategy exhibited a significantly longer duration of task execution, than the
rest of the prompts. Which is completely expected given the nature of different prompts. `think-step-by-step` as well as 
`polite-single-shot` is designed to respond with more tokens, which leads to longer response times and higher energy
consumption. On the other hand, `answer-only-no-expl` is designed to minimize the number of tokens generated, which
results in shorter response times and lower energy consumption. Thus, it explains the differences in energy consumption observed in Figure 1.
<table>
  <tr>
    <td align="center"><img src="./img/g2_cpu_power.png" alt="CPU Power Line Graph" width="450"/></td>
    <td align="center"><img src="./img/g2_gpu_power.png" alt="GPU Power Line Graph" width="450"/></td>
  </tr>
  <tr>
    <td align="center"><b>Figure 2a:</b> CPU Power Draw (mW) over time.</td>
    <td align="center"><b>Figure 2b:</b> GPU Power Draw (mW) over time.</td>
  </tr>
</table>

During our analysis, we also observed that the `think_step_by_step` seems to have a bit higher peaks in power draw for CPU
than for example `answer_only_no_expl`. Which leads us to another question: *Do certain prompting strategies require 
more CPU/GPU-intensive processing than others?* Our hypothesis is that the `think_step_by_step` strategy may require
more complex reasoning and thus more intensive processing, which could lead to higher power draw. To test it we decided
to look at the average energy per token for each strategy.

### 4.3 Energy Per Token
Our initial approach was to take total energy consumed and divide it by the total number of tokens generated.
<table>
  <tr>
    <td align="center"><img src="./img/g2_energy_token_profiling.png" alt="Average Energy per Token Bar Chart" width="450"/></td>
    <td align="center"><img src="./img/g2_energy_token_baseline.png" alt="Energy Difference per Token vs. Baseline" width="450"/></td>
  </tr>
  <tr>
    <td align="center"><b>Figure 2a:</b> Average Energy per Token by Prompt Strategy</td>
    <td align="center"><b>Figure 2b:</b> Energy Difference per Token vs. Baseline After Filtering Out Profiling Energy Draw</td>
  </tr>
</table>


### 4.4 Prompt Efficiency and Return on Investment
We looked at how much energy each prompt used compared to how good the generated code was. We measured quality in two ways: Levenshtein Ratio (how similar the code text is) and CodeBLEU (how well the logic matches).

#### The Best Trade-offs (Pareto Frontier)
To find the best balance, we plotted the average scores and drew a Pareto Frontier. The dotted line connects the optimal choices in our test setup.

![Pareto Frontier - Levenshtein](img/pareto_frontier_levenshtein.png)
*Figure 1: Pareto frontier showing the trade-off between energy and textual similarity (Levenshtein).*
![Pareto Frontier - CodeBLEU](img/pareto_frontier_codebleu.png)
*Figure 2: Pareto frontier showing the trade-off between energy and logical similarity (CodeBLEU).*

* The `think_step_by_step` prompt sits far in the top-left. In our setup, it consumed the most energy and resulted in the lowest quality.
* The dotted line shows our best options. `answer_only_no_expl` was the cheapest way to maintain baseline quality, while `polite_single_shot` cost just a little more energy but provided the highest logical quality (CodeBLEU).


#### Relative Impact (Is it Better and Cheaper?)

We also compared everything directly to the baseline to see which quadrant each strategy landed in during our runs.

![Relative Impact - Levenshtein](img/relative_change_levenshtein.png)
*Figure 3: Relative impact on Levenshtein quality and energy cost compared to the baseline prompt.*

![Relative Impact - CodeBLEU](img/relative_change_codebleu.png)
*Figure 4: Relative impact on CodeBLEU quality and energy cost compared to the baseline prompt.*

* `think_step_by_step` landed in the "Worse & Expensive" zone for both metrics. 
* `answer_only_no_expl` landed in the "Better & Cheaper" zone for Levenshtein, saving energy while keeping the same textual quality as the baseline.
* `polite_single_shot` gave a 50%+ boost to CodeBLEU logic for only a ~15% energy increase compared to the baseline.

#### Efficiency (Quality per 100 Joules)

Finally, we calculated the direct "bang for your buck" to see how many quality points the model scored for every 100 Joules it burned in our environment.

![Efficiency Ratio - Levenshtein](img/levenshtein_similarity_per_100_joules.png)
*Figure 5: Efficiency ratio showing Levenshtein points achieved per 100 Joules spent.*

![Efficiency Ratio - CodeBLEU](img/codebleu_similarity_per_100_joules.png)
*Figure 6: Efficiency ratio showing CodeBLEU points achieved per 100 Joules spent.*

* For basic code structure (Levenshtein), telling the model to skip explanations (`answer_only_no_expl`) gave the most points per Joule.
* For code logic (CodeBLEU), being polite (`polite_single_shot`) was the most efficient strategy in our tests.

#### Then, is the extra energy cost worth it?
Not necessarily. In our specific test setup, burning more energy did not guarantee better code. Forcing the model to explain itself (`think_step_by_step`) consumed a large amount of extra energy but actually lowered the code quality. For our environment, the better approach was to either force a direct answer (`answer_only_no_expl`) to save power, or add a polite phrase (`polite_single_shot`), which noticeably boosted code logic for an increase in energy cost.

---

## 5. Limitations (Antonio)
There are a few important limitations to keep in mind about how we setup this project:

* We only ran tests on `deepseek-coder-1.3b-instruct`. This is a small model. Massive, state-of-the-art models (like 70B+ parameter ones) might actually get a lot smarter when asked to "think step-by-step", which could make the extra energy cost worth it for them.
* To measure quality, we used formulas (Levenshtein and CodeBLEU) that check how similar the generated text is to the correct answer. We did not actually run the generated code to see if it compiles or passes tests, so a snippet could score highly but still have a hidden bug.
* We used the HumanEval dataset, which is basically just small, isolated Python puzzles. Real-world software engineering involves huge, complex files. Prompting strategies might use energy differently when dealing with a massive codebase.
* We only tested four exact prompt templates. Because LLMs are sensitive to phrasing, just changing "Think step-by-step" to "Let's work this out logically" could completely change the number of tokens the model spits out, which would change the energy footprint.
* The exact energy values (in Joules) we recorded only apply to the specific computer we used for testing. A different laptop or a massive server GPU will burn a totally different amount of power, even though the general trends should be similar.

---

## 6. Future Work (Pranav)